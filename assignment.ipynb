{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import pygame\n",
    "from typing import List\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\"\n",
    "            #    , render_mode=\"human\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"this is observation space: {type(env.observation_space)}\")\n",
    "print(f\"this is action space: {env.action_space}\")\n",
    "\n",
    "# 0: cart position 1: cart velocity 2: pole angle 3: pole angular velocity\n",
    "# observation high represents the highest value that the observation can take\n",
    "print(f\"this is observation space high: {env.observation_space.high}\")\n",
    "upper_bound = env.observation_space.high.tolist()\n",
    "\n",
    "# observation lowest represents the lowest value that the observation can take\n",
    "print(f\"this is observation space low: {env.observation_space.low}\")\n",
    "lower_bound = env.observation_space.low.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed() -> int:\n",
    "    return random.randint(0, 10000)\n",
    "\n",
    "seed = random_seed()\n",
    "print(f\"this is the seed: {seed}\")\n",
    "env.action_space.seed(seed)\n",
    "state = env.reset()[0]\n",
    "print(state)\n",
    "print(state[0])\n",
    "print(env.step(0)[0])\n",
    "\n",
    "# print(type(env))\n",
    "# print(env.action_space.n)\n",
    "# print(env.observation_space.shape)\n",
    "# print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: need to choose corresponding action according to state of the cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.1: getting discrete state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State space in cart pole is infinite, which means it is continuous\n",
    "\n",
    "hence we need to discretise the state space if not there will be infinite rows in the state space table\n",
    "\n",
    "We will need to discretise each state variable into n segments\n",
    "\n",
    "potentially have many different states being mapped into 1 state, but we can make the states finer inorder to reduce the amount of different states being sent into this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map continuous state to discrete indices for Q-matrix\n",
    "def discretiser(\n",
    "        state: List[float], \n",
    "        number_of_bins: List[int], \n",
    "        lower_bounds: List[float], \n",
    "        upper_bounds:List[float]\n",
    "        ) -> tuple:\n",
    "    \"\"\"\n",
    "    Discretise the continuous state into discrete indices for the Q-matrix\n",
    "    \n",
    "    Args:\n",
    "    state: List of continuous state variables\n",
    "    number_of_bins: Number of bins to divide each state variable\n",
    "    lower_bounds: Lower bounds for each state variable\n",
    "    upper_bounds: Upper bounds for each state variable\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of indices for each state variable\n",
    "    \"\"\"\n",
    "    cart_position_bin = np.linspace(lower_bounds[0], upper_bounds[0], number_of_bins[0])\n",
    "    cart_velocity_bin = np.linspace(lower_bounds[1], upper_bounds[1], number_of_bins[1])\n",
    "    pole_angle_bin = np.linspace(lower_bounds[2], upper_bounds[2], number_of_bins[2])\n",
    "    pole_angle_velocity_bin = np.linspace(lower_bounds[3], upper_bounds[3], number_of_bins[3])\n",
    "    \n",
    "    index_position = np.maximum(np.digitize(state[0], cart_position_bin) - 1, 0)\n",
    "    index_velocity = np.maximum(np.digitize(state[1], cart_velocity_bin) - 1, 0)\n",
    "    index_angle = np.maximum(np.digitize(state[2], pole_angle_bin) - 1, 0)\n",
    "    index_angular_velocity = np.maximum(np.digitize(state[3], pole_angle_velocity_bin) - 1, 0)\n",
    "    \n",
    "    return tuple([index_position, index_velocity, index_angle, index_angular_velocity])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.2: initialise Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-learning parameters and Q-matrix\n",
    "def initialize_q_learning(\n",
    "        env, \n",
    "        number_of_bins: List[int]\n",
    "        ) -> tuple:\n",
    "    \"\"\"\n",
    "    Initialize the Q-learning parameters and Q-matrix\n",
    "    \n",
    "    Args:\n",
    "    env: Gym environment\n",
    "    number_of_bins: Number of bins to divide each state variable\n",
    "    \n",
    "    Returns:\n",
    "    Q-matrix and list to store sum of rewards for each episode\n",
    "    \"\"\"\n",
    "    action_number = env.action_space.n\n",
    "    # Q-matrix initialized with random values between 0 and 1\n",
    "    q_matrix = np.random.uniform(low=0, high=1, \n",
    "                                size=(number_of_bins[0], number_of_bins[1], \n",
    "                                      number_of_bins[2], number_of_bins[3], \n",
    "                                      action_number))\n",
    "    # List to store sum of rewards for each episode\n",
    "    sum_rewards_episode = []\n",
    "    \n",
    "    return q_matrix, sum_rewards_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.3: selection of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(\n",
    "        state:tuple , \n",
    "        index: int, \n",
    "        q_matrix, \n",
    "        action_number: int, \n",
    "        epsilon: float, \n",
    "        number_of_bins: List[int], \n",
    "        lower_bounds: List[float], \n",
    "        upper_bounds: List[float]\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Select an action using the epsilon-greedy strategy.\n",
    "\n",
    "    Parameters:\n",
    "    state (array-like): The current state of the environment.\n",
    "    index (int): The current episode index.\n",
    "    q_matrix (ndarray): The Q-value matrix.\n",
    "    action_number (int): The total number of possible actions.\n",
    "    epsilon (float): The exploration rate.\n",
    "    number_of_bins (int): The number of bins for discretizing the state space.\n",
    "    lower_bounds (array-like): The lower bounds for each dimension of the state space.\n",
    "    upper_bounds (array-like): The upper bounds for each dimension of the state space.\n",
    "\n",
    "    Returns:\n",
    "    int: The selected action.\n",
    "    \"\"\"\n",
    "    # First 500 episodes: select completely random actions for exploration\n",
    "    if index < 500:\n",
    "        return np.random.choice(action_number)\n",
    "    \n",
    "    # After 7000 episodes: gradually decrease epsilon\n",
    "    if index > 7000:\n",
    "        epsilon = 0.999 * epsilon\n",
    "    \n",
    "    # Explore (random action) with probability epsilon\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(action_number)\n",
    "    \n",
    "    # Exploit (greedy action) with probability 1-epsilon\n",
    "    state_index = discretiser(state, number_of_bins, lower_bounds, upper_bounds)\n",
    "    # Find action(s) with maximum Q-value\n",
    "    max_actions = np.where(q_matrix[state_index] == np.max(q_matrix[state_index]))[0]\n",
    "    # Randomly choose one if multiple actions have the same max value\n",
    "    return np.random.choice(max_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episodes(\n",
    "        env, \n",
    "        alpha: float, \n",
    "        gamma: float, \n",
    "        epsilon: float, \n",
    "        number_episodes: int, \n",
    "        number_of_bins: List[int], \n",
    "        lower_bounds: List[float], \n",
    "        upper_bounds: List[float]\n",
    "        ) -> tuple:\n",
    "    \"\"\"\n",
    "    Simulates a number of episodes using Q-learning algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    env (gym.Env): The environment to simulate.\n",
    "    alpha (float): The learning rate.\n",
    "    gamma (float): The discount factor.\n",
    "    epsilon (float): The exploration rate.\n",
    "    number_episodes (int): The number of episodes to simulate.\n",
    "    number_of_bins (int): The number of bins to discretize the state space.\n",
    "    lower_bounds (list): The lower bounds for each state dimension.\n",
    "    upper_bounds (list): The upper bounds for each state dimension.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the Q-matrix and a list of sum of rewards per episode.\n",
    "    \"\"\"\n",
    "    q_matrix, sum_rewards_episode = initialize_q_learning(env, number_of_bins)\n",
    "    \n",
    "    for index_episode in range(number_episodes):\n",
    "        # List to store rewards for this episode\n",
    "        rewards_episode = []\n",
    "        \n",
    "        # Reset environment at the beginning of each episode\n",
    "        (state_s, _) = env.reset()\n",
    "        state_s = list(state_s)\n",
    "        \n",
    "        print(f\"Simulating episode {index_episode}\")\n",
    "        \n",
    "        # Run episode until terminal state is reached\n",
    "        terminal_state = False\n",
    "        while not terminal_state:\n",
    "            # Get the discretized index of current state\n",
    "            state_s_index = discretiser(state_s, number_of_bins, lower_bounds, upper_bounds)\n",
    "            \n",
    "            # Select action based on current state\n",
    "            action_a = select_action(state_s, index_episode, q_matrix, env.action_space.n, \n",
    "                                    epsilon, number_of_bins, lower_bounds, upper_bounds)\n",
    "            \n",
    "            # Take action and get next state and reward\n",
    "            (state_s_prime, reward, terminal_state, _, _) = env.step(action_a)\n",
    "            rewards_episode.append(reward)\n",
    "            state_s_prime = list(state_s_prime)\n",
    "            \n",
    "            # Get index for next state\n",
    "            state_s_prime_index = discretiser(state_s_prime, number_of_bins, lower_bounds, upper_bounds)\n",
    "            \n",
    "            # Get max Q-value for next state\n",
    "            q_max_prime = np.max(q_matrix[state_s_prime_index])\n",
    "            \n",
    "            # Update Q-matrix using Bellman equation\n",
    "            if not terminal_state:\n",
    "                error = reward + gamma * q_max_prime - q_matrix[state_s_index + (action_a,)]\n",
    "            else:\n",
    "                error = reward - q_matrix[state_s_index + (action_a,)]\n",
    "                \n",
    "            q_matrix[state_s_index + (action_a,)] = q_matrix[state_s_index + (action_a,)] + alpha * error\n",
    "            \n",
    "            # Set current state to next state\n",
    "            state_s = state_s_prime\n",
    "        \n",
    "        # Print and save episode rewards\n",
    "        print(f\"Sum of rewards {np.sum(rewards_episode)}\")\n",
    "        sum_rewards_episode.append(np.sum(rewards_episode))\n",
    "    \n",
    "    return q_matrix, sum_rewards_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate using the learned optimal policy\n",
    "def simulate_learned_strategy(\n",
    "        q_matrix, \n",
    "        number_of_bins: List[int], \n",
    "        lower_bounds :List[float], \n",
    "        upper_bounds: List[float]\n",
    "        ) -> tuple:\n",
    "    \"\"\"\n",
    "    Simulates the learned optimal policy using the Q-matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    q_matrix (np.ndarray): The Q-matrix learned from Q-learning.\n",
    "    number_of_bins (list): The number of bins for discretizing the state space.\n",
    "    lower_bounds (list): The lower bounds for each state dimension.\n",
    "    upper_bounds (list): The upper bounds for each state dimension.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the obtained rewards and the environment.\n",
    "    \"\"\"\n",
    "    env1 = gym.make('CartPole-v1', render_mode='human')\n",
    "    (current_state, _) = env1.reset()\n",
    "    env1.render()\n",
    "    time_steps = 1000\n",
    "    obtained_rewards = []\n",
    "    \n",
    "    for time_index in range(time_steps):\n",
    "        print(time_index)\n",
    "        # Select greedy action\n",
    "        state_index = discretiser(current_state, number_of_bins, lower_bounds, upper_bounds)\n",
    "        max_actions = np.where(q_matrix[state_index] == np.max(q_matrix[state_index]))[0]\n",
    "        action = np.random.choice(max_actions)\n",
    "        \n",
    "        current_state, reward, terminated, _, _ = env1.step(action)\n",
    "        obtained_rewards.append(reward)\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "    \n",
    "    return obtained_rewards, env1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(\n",
    "        env, alpha: float, \n",
    "        gamma: float, \n",
    "        epsilon: float, \n",
    "        number_episodes: int, \n",
    "        number_of_bins: List[int], \n",
    "        lower_bounds: List[float], \n",
    "        upper_bounds: List[float]\n",
    "        ) -> tuple:\n",
    "    \"\"\"\n",
    "    Train the Q-learning agent.\n",
    "    \n",
    "    Parameters:\n",
    "    env (gym.Env): The environment to train the agent on.\n",
    "    alpha (float): The learning rate.\n",
    "    gamma (float): The discount factor.\n",
    "    epsilon (float): The exploration rate.\n",
    "    number_episodes (int): The number of episodes to train the agent.\n",
    "    number_of_bins (list): The number of bins for discretizing the state space.\n",
    "    lower_bounds (list): The lower bounds for each state dimension.\n",
    "    upper_bounds (list): The upper bounds for each state dimension.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the Q-matrix and the rewards history.\n",
    "    \"\"\"\n",
    "    # Train the agent\n",
    "    print(\"Starting Q-learning training...\")\n",
    "    q_matrix, rewards_history = simulate_episodes(\n",
    "        env, alpha, gamma, epsilon, number_episodes, \n",
    "        number_of_bins, lower_bounds, upper_bounds\n",
    "    )\n",
    "    return q_matrix, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 2. Define Q-learning parameters\n",
    "alpha = 0.1             # Learning rate\n",
    "gamma = 0.99            # Discount factor\n",
    "epsilon = 0.3           # Initial exploration rate\n",
    "number_episodes = 8000  # Total number of training episodes\n",
    "\n",
    "# 3. Define discretisation parameters\n",
    "# Number of bins for discretizing each state dimension\n",
    "number_of_bins = [10, 10, 10, 10]  \n",
    "\n",
    "# Define bounds for state variables\n",
    "lower_bounds = [-2.4, -2, -0.2, -1.5]\n",
    "upper_bounds = [2.4, 2, 0.2, 1.5]\n",
    "\n",
    "# 4. Train the agent\n",
    "q_matrix, rewards_history = train_q_learning(\n",
    "    env, alpha, gamma, epsilon, number_episodes, \n",
    "    number_of_bins, lower_bounds, upper_bounds\n",
    ")\n",
    "\n",
    "# 5. Plot learning progress\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(rewards_history)\n",
    "plt.title('Learning Progress')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q_learning(q_matrix, env, number_of_bins, lower_bounds, upper_bounds):\n",
    "    # Test the trained agent\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        state_discrete = discretiser(state, number_of_bins, lower_bounds, upper_bounds)\n",
    "        action = np.argmax(q_matrix[state_discrete])\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "# 6. Test the trained agent\n",
    "total_reward = test_q_learning(q_matrix, env, number_of_bins, lower_bounds, upper_bounds)\n",
    "print(f'Total reward during testing: {total_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: run the RL agent 100 times, reset state at the start of each iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Render one episode played by the developed RL agent on Jupyter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC3000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
