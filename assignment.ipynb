{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import pygame\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\"\n",
    "            #    , render_mode=\"human\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"this is observation space: {type(env.observation_space)}\")\n",
    "print(f\"this is action space: {env.action_space}\")\n",
    "\n",
    "# 0: cart position 1: cart velocity 2: pole angle 3: pole angular velocity\n",
    "# observation high represents the highest value that the observation can take\n",
    "print(f\"this is observation space high: {env.observation_space.high}\")\n",
    "upper_bound = env.observation_space.high.tolist()\n",
    "\n",
    "# observation lowest represents the lowest value that the observation can take\n",
    "print(f\"this is observation space low: {env.observation_space.low}\")\n",
    "lower_bound = env.observation_space.low.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed() -> int:\n",
    "    return random.randint(0, 10000)\n",
    "\n",
    "seed = random_seed()\n",
    "print(f\"this is the seed: {seed}\")\n",
    "env.action_space.seed(seed)\n",
    "state = env.reset()[0]\n",
    "print(state)\n",
    "print(state[0])\n",
    "print(env.step(0)[0])\n",
    "\n",
    "# print(type(env))\n",
    "# print(env.action_space.n)\n",
    "# print(env.observation_space.shape)\n",
    "# print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: need to choose corresponding action according to state of the cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.1: getting discrete state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State space in cart pole is infinite, which means it is continuous\n",
    "\n",
    "hence we need to discretise the state space if not there will be infinite rows in the state space table\n",
    "\n",
    "We will need to discretise each state variable into n segments\n",
    "\n",
    "potentially have many different states being mapped into 1 state, but we can make the states finer inorder to reduce the amount of different states being sent into this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values inside the tuple for the number of segments in each bucket (the more the better)\n",
    "n_buckets = (10,10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretiser(\n",
    "        cart_position: float, \n",
    "        cart_velocity: float, \n",
    "        angle: float, \n",
    "        angular_velocity: float, \n",
    "        n_buckets: tuple = (10,10,10,10),\n",
    "        lower_bounds: list = env.observation_space.low.tolist(),\n",
    "        upper_bounds: list = env.observation_space.high.tolist(),\n",
    "    ) -> tuple:\n",
    "    \"\"\"\n",
    "    Discretises the continuous state space into discrete states for CartPole-v1.\n",
    "    \n",
    "    Parameters:\n",
    "    - cart_position (float): The cart's position.\n",
    "    - cart_velocity (float): The cart's velocity.\n",
    "    - angle (float): The pole's angle.\n",
    "    - angular_velocity (float): The pole's angular velocity.\n",
    "    - n_buckets (tuple): Number of bins for each state variable.\n",
    "    - lower_bounds (list): Lower bounds for each state variable. [OPTIONAL]\n",
    "    - upper_bounds (list): Upper bounds for each state variable. [OPTIONAL]\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Discretized indices for (cart_position, cart_velocity, angle, angular_velocity).\n",
    "    \"\"\"\n",
    "\n",
    "    # specify the width of each bucket for each state variable\n",
    "    bucket_width = [(upper_bounds[i] - lower_bounds[i]) / (n_buckets[i] - 1) for i in range(len(n_buckets))]\n",
    "    \n",
    "    # discretise the state space\n",
    "    cart_pos_index = int(min(max((cart_position - lower_bounds[0]) / bucket_width[0], 0), n_buckets[0] - 1))\n",
    "    cart_vel_index = int(min(max((cart_velocity - lower_bounds[1]) / bucket_width[1], 0), n_buckets[1] - 1))\n",
    "    angle_index = int(min(max((angle - lower_bounds[2]) / bucket_width[2], 0), n_buckets[2] - 1))\n",
    "    angular_vel_index = int(min(max((angular_velocity - lower_bounds[3]) / bucket_width[3], 0), n_buckets[3] - 1))\n",
    "\n",
    "\n",
    "    return (cart_pos_index, cart_vel_index, angle_index, angular_vel_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.2: Q-table definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.random.uniform(low=0, high=1, size=(n_buckets[0],n_buckets[1],n_buckets[2],n_buckets[3],env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.3: selection of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global epsilon\n",
    "\n",
    "def selectAction(state, index: int, epsilon: float) -> int:\n",
    "\n",
    "    # letting the first 300 episodes to do completely random actions to explore the environment\n",
    "    if index < 300:\n",
    "        return np.random.choice(env.action_space.n)\n",
    "    \n",
    "    # this number will be used for epsilon greedy strategy\n",
    "    random_number = np.random.random()\n",
    "\n",
    "    # after 7000 episodes, decrease the epsilon parameter\n",
    "    if index > 7000:\n",
    "        epsilon = 0.999 * epsilon\n",
    "\n",
    "    # if the condition is met, then choose a random action\n",
    "    if random_number < epsilon:\n",
    "        return np.random.choice(env.action_space.n)\n",
    "\n",
    "    # if the condition is not met, then choose the action with the highest Q value\n",
    "    else:\n",
    "        return np.random.choice(np.where(Q_table[state] == Q_table[state].max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episodes(number_of_episodes: int):\n",
    "    for index_episodes in range(number_of_episodes):\n",
    "\n",
    "        reward_episodes = []\n",
    "        state_s = env.reset()[0]\n",
    "\n",
    "        is_terminated = False\n",
    "        while not is_terminated:\n",
    "\n",
    "            state_s_index = discretiser(state_s[0], state_s[1], state_s[2], state_s[3])\n",
    "\n",
    "            action = selectAction(state_s_index, index_episodes, 0.9)\n",
    "            next_state, reward, is_terminated, _ = env.step(action)\n",
    "            reward_episodes.append(reward)\n",
    "\n",
    "            next_state_index = discretiser(next_state[0], next_state[1], next_state[2], next_state[3])\n",
    "\n",
    "            q_max_prime = np.max(Q_table[next_state_index])\n",
    "\n",
    "            if not is_terminated:\n",
    "                Q_table[state_s_index][action] += 0.1 * (reward + 0.99 * q_max_prime - Q_table[state_s_index][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: run the RL agent 100 times, reset state at the start of each iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Render one episode played by the developed RL agent on Jupyter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC3000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
