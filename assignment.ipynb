{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import pygame\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\"\n",
    "            #    , render_mode=\"human\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"this is observation space: {type(env.observation_space)}\")\n",
    "print(f\"this is action space: {env.action_space}\")\n",
    "\n",
    "# 0: cart position 1: cart velocity 2: pole angle 3: pole angular velocity\n",
    "# observation high represents the highest value that the observation can take\n",
    "print(f\"this is observation space high: {env.observation_space.high}\")\n",
    "upper_bound = env.observation_space.high.tolist()\n",
    "\n",
    "# observation lowest represents the lowest value that the observation can take\n",
    "print(f\"this is observation space low: {env.observation_space.low}\")\n",
    "lower_bound = env.observation_space.low.tolist()\n",
    "\n",
    "# potentially changing\n",
    "span = [(upper_bound[i] - lower_bound[i]) for i in range(len(upper_bound))]\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed() -> int:\n",
    "    return random.randint(0, 10000)\n",
    "\n",
    "seed = random_seed()\n",
    "print(f\"this is the seed: {seed}\")\n",
    "env.action_space.seed(seed)\n",
    "state = env.reset()[0]\n",
    "print(state)\n",
    "print(state[0])\n",
    "print(env.step(0))\n",
    "print(type(env))\n",
    "# print(env.action_space.n)\n",
    "# print(env.observation_space.shape)\n",
    "# print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: need to choose corresponding action according to state of the cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1.1: getting discrete state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretiser(\n",
    "        cart_position: float, \n",
    "        cart_velocity: float, \n",
    "        angle: float, \n",
    "        angular_velocity: float, \n",
    "        n_buckets: tuple = (10,10,10,10),\n",
    "        lower_bounds: list = env.observation_space.low.tolist(),\n",
    "        upper_bounds: list = env.observation_space.high.tolist(),\n",
    "    ) -> tuple:\n",
    "    \"\"\"\n",
    "    Discretises the continuous state space into discrete states for CartPole-v1.\n",
    "    \n",
    "    Parameters:\n",
    "    - cart_position (float): The cart's position.\n",
    "    - cart_velocity (float): The cart's velocity.\n",
    "    - angle (float): The pole's angle.\n",
    "    - angular_velocity (float): The pole's angular velocity.\n",
    "    - bins (tuple): Number of bins for each state variable.\n",
    "    - lower_bounds (list): Lower bounds for each state variable. [OPTIONAL]\n",
    "    - upper_bounds (list): Upper bounds for each state variable. [OPTIONAL]\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Discretized indices for (cart_position, cart_velocity, angle, angular_velocity).\n",
    "    \"\"\"\n",
    "\n",
    "    bucket_width = [(upper_bounds[i] - lower_bounds[i]) / (n_buckets[i] - 1) for i in range(len(n_buckets))]\n",
    "    \n",
    "    cart_pos_index = int(min(max((cart_position - lower_bounds[0]) / bucket_width[0], 0), n_buckets[0] - 1))\n",
    "    cart_vel_index = int(min(max((cart_velocity - lower_bounds[1]) / bucket_width[1], 0), n_buckets[1] - 1))\n",
    "    angle_index = int(min(max((angle - lower_bounds[2]) / bucket_width[2], 0), n_buckets[2] - 1))\n",
    "    angular_vel_index = int(min(max((angular_velocity - lower_bounds[3]) / bucket_width[3], 0), n_buckets[3] - 1))\n",
    "\n",
    "\n",
    "    return (cart_pos_index, cart_vel_index, angle_index, angular_vel_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: run the RL agent 100 times, reset state at the start of each iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Render one episode played by the developed RL agent on Jupyter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC3000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
