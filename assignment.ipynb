{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import pygame\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\"\n",
    "            #    , render_mode=\"human\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is observation space: <class 'gym.spaces.box.Box'>\n",
      "this is action space: Discrete(2)\n",
      "this is observation space high: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "this is observation space low: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "[9.600000381469727, 6.805646932770577e+38, 0.8377580642700195, 6.805646932770577e+38]\n"
     ]
    }
   ],
   "source": [
    "print(f\"this is observation space: {type(env.observation_space)}\")\n",
    "print(f\"this is action space: {env.action_space}\")\n",
    "\n",
    "# 0: cart position 1: cart velocity 2: pole angle 3: pole angular velocity\n",
    "# observation high represents the highest value that the observation can take\n",
    "print(f\"this is observation space high: {env.observation_space.high}\")\n",
    "upper_bound = env.observation_space.high.tolist()\n",
    "\n",
    "# observation lowest represents the lowest value that the observation can take\n",
    "print(f\"this is observation space low: {env.observation_space.low}\")\n",
    "lower_bound = env.observation_space.low.tolist()\n",
    "\n",
    "# potentially changing\n",
    "span = [(upper_bound[i] - lower_bound[i]) for i in range(len(upper_bound))]\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the seed: 7629\n",
      "[ 0.02753599 -0.04742703 -0.02779591  0.04869635]\n",
      "0.027535994\n",
      "(array([ 0.02658745, -0.24213961, -0.02682198,  0.33248147], dtype=float32), 1.0, False, False, {})\n",
      "<class 'gym.wrappers.time_limit.TimeLimit'>\n"
     ]
    }
   ],
   "source": [
    "def random_seed() -> int:\n",
    "    return random.randint(0, 10000)\n",
    "\n",
    "seed = random_seed()\n",
    "print(f\"this is the seed: {seed}\")\n",
    "env.action_space.seed(seed)\n",
    "state = env.reset()[0]\n",
    "print(state)\n",
    "print(state[0])\n",
    "print(env.step(0))\n",
    "print(type(env))\n",
    "# print(env.action_space.n)\n",
    "# print(env.observation_space.shape)\n",
    "# print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depreciated\n",
    "def get_state(state: np.ndarray):\n",
    "    cart_position = state[0]\n",
    "    cart_velocity = state[1]\n",
    "    pole_angle = state[2] \n",
    "    pole_angular_velocity = state[3]\n",
    "    return cart_position, cart_velocity, pole_angle, pole_angular_velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: need to choose corresponding action according to state of the cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depreciated for now\n",
    "def rewards(action: int, cur_pos: List[float], next_pos: List[float]) -> int:\n",
    "    reward = 0\n",
    "\n",
    "    # action based on current state\n",
    "    if action == 0:\n",
    "        if cur_pos[0] > 0.5:\n",
    "            reward += 1\n",
    "        if cur_pos[1] > 0.25:\n",
    "            reward += 1\n",
    "        if cur_pos[2] > 0:\n",
    "            reward -= 100       # punishment for pole angle if it goes in wrong direction\n",
    "        if cur_pos[3] > 0.5:\n",
    "            reward -= 100       # punishment for pole angular velocity if it goes in wrong direction\n",
    "    elif action == 1:\n",
    "        if cur_pos[0] < -0.5:\n",
    "            reward += 1\n",
    "        if cur_pos[1] < -0.25:\n",
    "            reward += 1\n",
    "        if cur_pos[2] < 0:\n",
    "            reward -= 100\n",
    "        if cur_pos[3] < -0.5:\n",
    "            reward -= 100\n",
    "\n",
    "    # next state\n",
    "    reward += 1 if (-.25 < next_pos[0] < .25) else (0 if (-.5 < next_pos[0] < .5) else -1)\n",
    "    reward += 1 if (-.1 < next_pos[1] < .1) else (0 if (-.25 < next_pos[1] < .25) else -1)\n",
    "    reward += 5 if (-.05 < next_pos[2] < .05) else (0 if (-.1 < next_pos[2] < .1) else -5)\n",
    "    reward += 5 if (-.5 < next_pos[3] < .5) else (0 if (-1 < next_pos[3] < 1) else -5)\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy: dict, env , gamma: float = 0.99, theta: float = 1e-6) -> np.ndarray:\n",
    "    V = np.zeros(env.observation_space.shape[0])\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.observation_space.shape[0]):\n",
    "            v = 0\n",
    "            action = policy[state]\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = next_state.tolist()\n",
    "            v += reward + gamma * V[state]\n",
    "            delta = max(delta, np.abs(v - V[state]))\n",
    "            V[state] = v\n",
    "            env.reset()\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V: np.ndarray, env, gamma: float = 0.99) -> dict:\n",
    "    policy = {}\n",
    "    for state in range(env.observation_space.shape[0]):\n",
    "        Q = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = next_state.tolist()\n",
    "            Q[action] = reward + gamma * V[state]\n",
    "            env.reset()\n",
    "        policy[state] = np.argmax(Q)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0}\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration\n",
    "# the greek characters are generated from https://www.toptal.com/designers/htmlarrows/math/\n",
    "# π(s) = argmax_a Σ_s' P(s' | s, a) * (R(s) + γV(s'))\n",
    "\n",
    "def policy_iteration(env, gamma: float = 0.99, theta: float = 1e-6) -> dict:\n",
    "    policy = {state: env.action_space.sample() for state in range(env.observation_space.shape[0])}\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, env, gamma, theta)\n",
    "        new_policy = policy_improvement(V, env, gamma)\n",
    "        if new_policy == policy:\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "# Train the policy using policy iteration\n",
    "optimal_policy = policy_iteration(env)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### optimal policy will be after running policy iterations, save it to a file and gitignore it, then send the file on tele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: run the RL agent 100 times, reset state at the start of each iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Render one episode played by the developed RL agent on Jupyter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SC3000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
